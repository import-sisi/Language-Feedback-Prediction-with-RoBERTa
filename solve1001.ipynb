{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-12T15:50:54.742166Z",
     "iopub.status.busy": "2023-05-12T15:50:54.741439Z",
     "iopub.status.idle": "2023-05-12T15:51:09.784746Z",
     "shell.execute_reply": "2023-05-12T15:51:09.783661Z",
     "shell.execute_reply.started": "2023-05-12T15:50:54.742132Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/feedback-prize-english-language-learning/sample_submission.csv\n",
      "/kaggle/input/feedback-prize-english-language-learning/train.csv\n",
      "/kaggle/input/feedback-prize-english-language-learning/test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertPreTrainedModel\n",
    "from transformers import BertConfig, BertModel, AdamW\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import seaborn as sns\n",
    "import random \n",
    "import datetime\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T15:51:12.506614Z",
     "iopub.status.busy": "2023-05-12T15:51:12.505903Z",
     "iopub.status.idle": "2023-05-12T15:51:12.704433Z",
     "shell.execute_reply": "2023-05-12T15:51:12.703438Z",
     "shell.execute_reply.started": "2023-05-12T15:51:12.506574Z"
    }
   },
   "outputs": [],
   "source": [
    "dirname = '/kaggle/input/feedback-prize-english-language-learning'\n",
    "train = pd.read_csv(dirname + '/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T15:51:21.159496Z",
     "iopub.status.busy": "2023-05-12T15:51:21.159118Z",
     "iopub.status.idle": "2023-05-12T15:51:21.177461Z",
     "shell.execute_reply": "2023-05-12T15:51:21.176342Z",
     "shell.execute_reply.started": "2023-05-12T15:51:21.159465Z"
    }
   },
   "outputs": [],
   "source": [
    "train.drop(columns=['text_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T15:51:58.817671Z",
     "iopub.status.busy": "2023-05-12T15:51:58.817326Z",
     "iopub.status.idle": "2023-05-12T15:51:58.886250Z",
     "shell.execute_reply": "2023-05-12T15:51:58.885151Z",
     "shell.execute_reply.started": "2023-05-12T15:51:58.817644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "train_df, valid_df = train_test_split(train, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T15:52:09.171967Z",
     "iopub.status.busy": "2023-05-12T15:52:09.171366Z",
     "iopub.status.idle": "2023-05-12T15:52:09.186747Z",
     "shell.execute_reply": "2023-05-12T15:52:09.185756Z",
     "shell.execute_reply.started": "2023-05-12T15:52:09.171906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           full_text  cohesion  syntax  \\\n",
      "0  I think that students would benefit from learn...       3.5     3.5   \n",
      "1  When a problem is a change you have to let it ...       2.5     2.5   \n",
      "2  Dear, Principal\\n\\nIf u change the school poli...       3.0     3.5   \n",
      "3  The best time in life is when you become yours...       4.5     4.5   \n",
      "4  Small act of kindness can impact in other peop...       2.5     3.0   \n",
      "\n",
      "   vocabulary  phraseology  grammar  conventions  \n",
      "0         3.0          3.0      4.0          3.0  \n",
      "1         3.0          2.0      2.0          2.5  \n",
      "2         3.0          3.0      3.0          2.5  \n",
      "3         4.5          4.5      4.0          5.0  \n",
      "4         3.0          3.0      2.5          2.5  \n"
     ]
    }
   ],
   "source": [
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T15:53:56.873556Z",
     "iopub.status.busy": "2023-05-12T15:53:56.873116Z",
     "iopub.status.idle": "2023-05-12T15:53:57.097819Z",
     "shell.execute_reply": "2023-05-12T15:53:57.096801Z",
     "shell.execute_reply.started": "2023-05-12T15:53:56.873512Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path_or_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T15:54:15.015816Z",
     "iopub.status.busy": "2023-05-12T15:54:15.015170Z",
     "iopub.status.idle": "2023-05-12T15:54:15.027995Z",
     "shell.execute_reply": "2023-05-12T15:54:15.026984Z",
     "shell.execute_reply.started": "2023-05-12T15:54:15.015785Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = []\n",
    "        self.max_length = max_length\n",
    "        for i, row in dataframe.iterrows():\n",
    "            text = row['full_text']\n",
    "            cohesion = row['cohesion']\n",
    "            syntax = row['syntax']\n",
    "            vocabulary = row['vocabulary']\n",
    "            phraseology = row['phraseology']\n",
    "            grammar = row['grammar']\n",
    "            conventions = row['conventions']\n",
    "            target = [cohesion, syntax, vocabulary, phraseology, grammar, conventions]\n",
    "            self.data.append((text, target))\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, targets = self.data[idx]\n",
    "\n",
    "        input = self.tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "        input_ids = input['input_ids'].squeeze(0)\n",
    "        attention_mask = input['attention_mask'].squeeze(0)\n",
    "        targets = torch.tensor([float(target) for target in targets])\n",
    "\n",
    "        return input_ids, attention_mask, targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T15:54:21.939124Z",
     "iopub.status.busy": "2023-05-12T15:54:21.938545Z",
     "iopub.status.idle": "2023-05-12T15:54:22.268064Z",
     "shell.execute_reply": "2023-05-12T15:54:22.267154Z",
     "shell.execute_reply.started": "2023-05-12T15:54:21.939090Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df, tokenizer)\n",
    "val_dataset = CustomDataset(valid_df, tokenizer)\n",
    "\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T15:54:27.474715Z",
     "iopub.status.busy": "2023-05-12T15:54:27.474365Z",
     "iopub.status.idle": "2023-05-12T15:54:27.483408Z",
     "shell.execute_reply": "2023-05-12T15:54:27.482487Z",
     "shell.execute_reply.started": "2023-05-12T15:54:27.474685Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertForMultipleRegression(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 6)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.regressor(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T15:54:32.672869Z",
     "iopub.status.busy": "2023-05-12T15:54:32.672521Z",
     "iopub.status.idle": "2023-05-12T15:54:32.681820Z",
     "shell.execute_reply": "2023-05-12T15:54:32.680954Z",
     "shell.execute_reply.started": "2023-05-12T15:54:32.672841Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T15:54:38.914112Z",
     "iopub.status.busy": "2023-05-12T15:54:38.913698Z",
     "iopub.status.idle": "2023-05-12T15:54:48.113563Z",
     "shell.execute_reply": "2023-05-12T15:54:48.112601Z",
     "shell.execute_reply.started": "2023-05-12T15:54:38.914082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bf82bd260241e6a5196732261e7058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleRegression: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultipleRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultipleRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultipleRegression were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['regressor.bias', 'regressor.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForMultipleRegression.from_pretrained(model_path_or_name)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T15:55:09.712911Z",
     "iopub.status.busy": "2023-05-12T15:55:09.712563Z",
     "iopub.status.idle": "2023-05-12T15:55:09.725850Z",
     "shell.execute_reply": "2023-05-12T15:55:09.724849Z",
     "shell.execute_reply.started": "2023-05-12T15:55:09.712881Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10 \n",
    "optimizer = AdamW (model.parameters(),\n",
    "                  lr = 1e-6,\n",
    "                  eps = 1e-8,\n",
    "                )\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T15:55:23.475859Z",
     "iopub.status.busy": "2023-05-12T15:55:23.475505Z",
     "iopub.status.idle": "2023-05-12T15:55:23.482659Z",
     "shell.execute_reply": "2023-05-12T15:55:23.481616Z",
     "shell.execute_reply.started": "2023-05-12T15:55:23.475828Z"
    }
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "show_every = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T15:55:37.926722Z",
     "iopub.status.busy": "2023-05-12T15:55:37.925940Z",
     "iopub.status.idle": "2023-05-12T16:58:11.480151Z",
     "shell.execute_reply": "2023-05-12T16:58:11.479115Z",
     "shell.execute_reply.started": "2023-05-12T15:55:37.926686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Batch 20 / 440.  Elapsed: 0:00:18.\n",
      "Training loss: 9.375\n",
      "  Batch 40 / 440.  Elapsed: 0:00:34.\n",
      "Training loss: 8.322\n",
      "  Batch 60 / 440.  Elapsed: 0:00:49.\n",
      "Training loss: 8.235\n",
      "  Batch 80 / 440.  Elapsed: 0:01:06.\n",
      "Training loss: 7.889\n",
      "  Batch 100 / 440.  Elapsed: 0:01:23.\n",
      "Training loss: 7.910\n",
      "  Batch 120 / 440.  Elapsed: 0:01:40.\n",
      "Training loss: 7.376\n",
      "  Batch 140 / 440.  Elapsed: 0:01:56.\n",
      "Training loss: 7.052\n",
      "  Batch 160 / 440.  Elapsed: 0:02:12.\n",
      "Training loss: 6.251\n",
      "  Batch 180 / 440.  Elapsed: 0:02:29.\n",
      "Training loss: 5.893\n",
      "  Batch 200 / 440.  Elapsed: 0:02:45.\n",
      "Training loss: 5.239\n",
      "  Batch 220 / 440.  Elapsed: 0:03:02.\n",
      "Training loss: 4.890\n",
      "  Batch 240 / 440.  Elapsed: 0:03:18.\n",
      "Training loss: 4.052\n",
      "  Batch 260 / 440.  Elapsed: 0:03:35.\n",
      "Training loss: 3.843\n",
      "  Batch 280 / 440.  Elapsed: 0:03:51.\n",
      "Training loss: 3.623\n",
      "  Batch 300 / 440.  Elapsed: 0:04:07.\n",
      "Training loss: 3.015\n",
      "  Batch 320 / 440.  Elapsed: 0:04:24.\n",
      "Training loss: 2.632\n",
      "  Batch 340 / 440.  Elapsed: 0:04:40.\n",
      "Training loss: 2.526\n",
      "  Batch 360 / 440.  Elapsed: 0:04:57.\n",
      "Training loss: 2.733\n",
      "  Batch 380 / 440.  Elapsed: 0:05:13.\n",
      "Training loss: 2.554\n",
      "  Batch 400 / 440.  Elapsed: 0:05:30.\n",
      "Training loss: 2.376\n",
      "  Batch 420 / 440.  Elapsed: 0:05:46.\n",
      "Training loss: 2.177\n",
      "  Training epoch took: 0:06:02\n",
      "\n",
      "Running Validation...\n",
      "Epoch 0: Train Loss: 5.0276, Validation Loss: 1.6608\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch 20 / 440.  Elapsed: 0:00:17.\n",
      "Training loss: 1.939\n",
      "  Batch 40 / 440.  Elapsed: 0:00:34.\n",
      "Training loss: 1.840\n",
      "  Batch 60 / 440.  Elapsed: 0:00:50.\n",
      "Training loss: 1.903\n",
      "  Batch 80 / 440.  Elapsed: 0:01:07.\n",
      "Training loss: 1.567\n",
      "  Batch 100 / 440.  Elapsed: 0:01:23.\n",
      "Training loss: 1.528\n",
      "  Batch 120 / 440.  Elapsed: 0:01:40.\n",
      "Training loss: 1.509\n",
      "  Batch 140 / 440.  Elapsed: 0:01:56.\n",
      "Training loss: 1.517\n",
      "  Batch 160 / 440.  Elapsed: 0:02:12.\n",
      "Training loss: 1.362\n",
      "  Batch 180 / 440.  Elapsed: 0:02:29.\n",
      "Training loss: 1.294\n",
      "  Batch 200 / 440.  Elapsed: 0:02:45.\n",
      "Training loss: 1.191\n",
      "  Batch 220 / 440.  Elapsed: 0:03:02.\n",
      "Training loss: 1.144\n",
      "  Batch 240 / 440.  Elapsed: 0:03:18.\n",
      "Training loss: 1.020\n",
      "  Batch 260 / 440.  Elapsed: 0:03:34.\n",
      "Training loss: 0.983\n",
      "  Batch 280 / 440.  Elapsed: 0:03:51.\n",
      "Training loss: 0.961\n",
      "  Batch 300 / 440.  Elapsed: 0:04:07.\n",
      "Training loss: 0.954\n",
      "  Batch 320 / 440.  Elapsed: 0:04:24.\n",
      "Training loss: 0.896\n",
      "  Batch 340 / 440.  Elapsed: 0:04:40.\n",
      "Training loss: 0.750\n",
      "  Batch 360 / 440.  Elapsed: 0:04:57.\n",
      "Training loss: 0.831\n",
      "  Batch 380 / 440.  Elapsed: 0:05:13.\n",
      "Training loss: 0.604\n",
      "  Batch 400 / 440.  Elapsed: 0:05:29.\n",
      "Training loss: 0.811\n",
      "  Batch 420 / 440.  Elapsed: 0:05:46.\n",
      "Training loss: 0.673\n",
      "  Training epoch took: 0:06:02\n",
      "\n",
      "Running Validation...\n",
      "Epoch 1: Train Loss: 1.1818, Validation Loss: 0.5211\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch 20 / 440.  Elapsed: 0:00:17.\n",
      "Training loss: 0.493\n",
      "  Batch 40 / 440.  Elapsed: 0:00:34.\n",
      "Training loss: 0.608\n",
      "  Batch 60 / 440.  Elapsed: 0:00:50.\n",
      "Training loss: 0.518\n",
      "  Batch 80 / 440.  Elapsed: 0:01:06.\n",
      "Training loss: 0.613\n",
      "  Batch 100 / 440.  Elapsed: 0:01:23.\n",
      "Training loss: 0.553\n",
      "  Batch 120 / 440.  Elapsed: 0:01:39.\n",
      "Training loss: 0.579\n",
      "  Batch 140 / 440.  Elapsed: 0:01:56.\n",
      "Training loss: 0.525\n",
      "  Batch 160 / 440.  Elapsed: 0:02:12.\n",
      "Training loss: 0.438\n",
      "  Batch 180 / 440.  Elapsed: 0:02:29.\n",
      "Training loss: 0.539\n",
      "  Batch 200 / 440.  Elapsed: 0:02:45.\n",
      "Training loss: 0.509\n",
      "  Batch 220 / 440.  Elapsed: 0:03:01.\n",
      "Training loss: 0.484\n",
      "  Batch 240 / 440.  Elapsed: 0:03:18.\n",
      "Training loss: 0.510\n",
      "  Batch 260 / 440.  Elapsed: 0:03:34.\n",
      "Training loss: 0.396\n",
      "  Batch 280 / 440.  Elapsed: 0:03:51.\n",
      "Training loss: 0.429\n",
      "  Batch 300 / 440.  Elapsed: 0:04:07.\n",
      "Training loss: 0.429\n",
      "  Batch 320 / 440.  Elapsed: 0:04:24.\n",
      "Training loss: 0.447\n",
      "  Batch 340 / 440.  Elapsed: 0:04:40.\n",
      "Training loss: 0.467\n",
      "  Batch 360 / 440.  Elapsed: 0:04:57.\n",
      "Training loss: 0.464\n",
      "  Batch 380 / 440.  Elapsed: 0:05:13.\n",
      "Training loss: 0.504\n",
      "  Batch 400 / 440.  Elapsed: 0:05:29.\n",
      "Training loss: 0.409\n",
      "  Batch 420 / 440.  Elapsed: 0:05:46.\n",
      "Training loss: 0.470\n",
      "  Training epoch took: 0:06:01\n",
      "\n",
      "Running Validation...\n",
      "Epoch 2: Train Loss: 0.4925, Validation Loss: 0.3844\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch 20 / 440.  Elapsed: 0:00:17.\n",
      "Training loss: 0.421\n",
      "  Batch 40 / 440.  Elapsed: 0:00:34.\n",
      "Training loss: 0.387\n",
      "  Batch 60 / 440.  Elapsed: 0:00:50.\n",
      "Training loss: 0.398\n",
      "  Batch 80 / 440.  Elapsed: 0:01:07.\n",
      "Training loss: 0.391\n",
      "  Batch 100 / 440.  Elapsed: 0:01:23.\n",
      "Training loss: 0.349\n",
      "  Batch 120 / 440.  Elapsed: 0:01:39.\n",
      "Training loss: 0.416\n",
      "  Batch 140 / 440.  Elapsed: 0:01:56.\n",
      "Training loss: 0.440\n",
      "  Batch 160 / 440.  Elapsed: 0:02:12.\n",
      "Training loss: 0.373\n",
      "  Batch 180 / 440.  Elapsed: 0:02:29.\n",
      "Training loss: 0.389\n",
      "  Batch 200 / 440.  Elapsed: 0:02:45.\n",
      "Training loss: 0.356\n",
      "  Batch 220 / 440.  Elapsed: 0:03:01.\n",
      "Training loss: 0.354\n",
      "  Batch 240 / 440.  Elapsed: 0:03:18.\n",
      "Training loss: 0.292\n",
      "  Batch 260 / 440.  Elapsed: 0:03:34.\n",
      "Training loss: 0.393\n",
      "  Batch 280 / 440.  Elapsed: 0:03:51.\n",
      "Training loss: 0.332\n",
      "  Batch 300 / 440.  Elapsed: 0:04:07.\n",
      "Training loss: 0.419\n",
      "  Batch 320 / 440.  Elapsed: 0:04:23.\n",
      "Training loss: 0.384\n",
      "  Batch 340 / 440.  Elapsed: 0:04:40.\n",
      "Training loss: 0.346\n",
      "  Batch 360 / 440.  Elapsed: 0:04:56.\n",
      "Training loss: 0.361\n",
      "  Batch 380 / 440.  Elapsed: 0:05:13.\n",
      "Training loss: 0.373\n",
      "  Batch 400 / 440.  Elapsed: 0:05:29.\n",
      "Training loss: 0.304\n",
      "  Batch 420 / 440.  Elapsed: 0:05:45.\n",
      "Training loss: 0.334\n",
      "  Training epoch took: 0:06:01\n",
      "\n",
      "Running Validation...\n",
      "Epoch 3: Train Loss: 0.3696, Validation Loss: 0.3470\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch 20 / 440.  Elapsed: 0:00:17.\n",
      "Training loss: 0.323\n",
      "  Batch 40 / 440.  Elapsed: 0:00:34.\n",
      "Training loss: 0.330\n",
      "  Batch 60 / 440.  Elapsed: 0:00:50.\n",
      "Training loss: 0.299\n",
      "  Batch 80 / 440.  Elapsed: 0:01:07.\n",
      "Training loss: 0.310\n",
      "  Batch 100 / 440.  Elapsed: 0:01:23.\n",
      "Training loss: 0.355\n",
      "  Batch 120 / 440.  Elapsed: 0:01:39.\n",
      "Training loss: 0.363\n",
      "  Batch 140 / 440.  Elapsed: 0:01:56.\n",
      "Training loss: 0.310\n",
      "  Batch 160 / 440.  Elapsed: 0:02:12.\n",
      "Training loss: 0.329\n",
      "  Batch 180 / 440.  Elapsed: 0:02:28.\n",
      "Training loss: 0.316\n",
      "  Batch 200 / 440.  Elapsed: 0:02:45.\n",
      "Training loss: 0.300\n",
      "  Batch 220 / 440.  Elapsed: 0:03:01.\n",
      "Training loss: 0.342\n",
      "  Batch 240 / 440.  Elapsed: 0:03:18.\n",
      "Training loss: 0.311\n",
      "  Batch 260 / 440.  Elapsed: 0:03:34.\n",
      "Training loss: 0.340\n",
      "  Batch 280 / 440.  Elapsed: 0:03:51.\n",
      "Training loss: 0.364\n",
      "  Batch 300 / 440.  Elapsed: 0:04:07.\n",
      "Training loss: 0.321\n",
      "  Batch 320 / 440.  Elapsed: 0:04:24.\n",
      "Training loss: 0.314\n",
      "  Batch 340 / 440.  Elapsed: 0:04:40.\n",
      "Training loss: 0.338\n",
      "  Batch 360 / 440.  Elapsed: 0:04:56.\n",
      "Training loss: 0.362\n",
      "  Batch 380 / 440.  Elapsed: 0:05:13.\n",
      "Training loss: 0.318\n",
      "  Batch 400 / 440.  Elapsed: 0:05:29.\n",
      "Training loss: 0.289\n",
      "  Batch 420 / 440.  Elapsed: 0:05:46.\n",
      "Training loss: 0.308\n",
      "  Training epoch took: 0:06:01\n",
      "\n",
      "Running Validation...\n",
      "Epoch 4: Train Loss: 0.3241, Validation Loss: 0.3435\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch 20 / 440.  Elapsed: 0:00:17.\n",
      "Training loss: 0.306\n",
      "  Batch 40 / 440.  Elapsed: 0:00:34.\n",
      "Training loss: 0.304\n",
      "  Batch 60 / 440.  Elapsed: 0:00:50.\n",
      "Training loss: 0.324\n",
      "  Batch 80 / 440.  Elapsed: 0:01:07.\n",
      "Training loss: 0.278\n",
      "  Batch 100 / 440.  Elapsed: 0:01:23.\n",
      "Training loss: 0.306\n",
      "  Batch 120 / 440.  Elapsed: 0:01:39.\n",
      "Training loss: 0.307\n",
      "  Batch 140 / 440.  Elapsed: 0:01:56.\n",
      "Training loss: 0.302\n",
      "  Batch 160 / 440.  Elapsed: 0:02:12.\n",
      "Training loss: 0.295\n",
      "  Batch 180 / 440.  Elapsed: 0:02:28.\n",
      "Training loss: 0.293\n",
      "  Batch 200 / 440.  Elapsed: 0:02:45.\n",
      "Training loss: 0.338\n",
      "  Batch 220 / 440.  Elapsed: 0:03:01.\n",
      "Training loss: 0.290\n",
      "  Batch 240 / 440.  Elapsed: 0:03:18.\n",
      "Training loss: 0.311\n",
      "  Batch 260 / 440.  Elapsed: 0:03:34.\n",
      "Training loss: 0.318\n",
      "  Batch 280 / 440.  Elapsed: 0:03:50.\n",
      "Training loss: 0.279\n",
      "  Batch 300 / 440.  Elapsed: 0:04:07.\n",
      "Training loss: 0.271\n",
      "  Batch 320 / 440.  Elapsed: 0:04:23.\n",
      "Training loss: 0.296\n",
      "  Batch 340 / 440.  Elapsed: 0:04:40.\n",
      "Training loss: 0.309\n",
      "  Batch 360 / 440.  Elapsed: 0:04:56.\n",
      "Training loss: 0.312\n",
      "  Batch 380 / 440.  Elapsed: 0:05:13.\n",
      "Training loss: 0.293\n",
      "  Batch 400 / 440.  Elapsed: 0:05:29.\n",
      "Training loss: 0.255\n",
      "  Batch 420 / 440.  Elapsed: 0:05:45.\n",
      "Training loss: 0.334\n",
      "  Training epoch took: 0:06:01\n",
      "\n",
      "Running Validation...\n",
      "Epoch 5: Train Loss: 0.3006, Validation Loss: 0.3123\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch 20 / 440.  Elapsed: 0:00:17.\n",
      "Training loss: 0.318\n",
      "  Batch 40 / 440.  Elapsed: 0:00:34.\n",
      "Training loss: 0.319\n",
      "  Batch 60 / 440.  Elapsed: 0:00:50.\n",
      "Training loss: 0.271\n",
      "  Batch 80 / 440.  Elapsed: 0:01:06.\n",
      "Training loss: 0.299\n",
      "  Batch 100 / 440.  Elapsed: 0:01:23.\n",
      "Training loss: 0.292\n",
      "  Batch 120 / 440.  Elapsed: 0:01:39.\n",
      "Training loss: 0.267\n",
      "  Batch 140 / 440.  Elapsed: 0:01:56.\n",
      "Training loss: 0.301\n",
      "  Batch 160 / 440.  Elapsed: 0:02:12.\n",
      "Training loss: 0.254\n",
      "  Batch 180 / 440.  Elapsed: 0:02:28.\n",
      "Training loss: 0.266\n",
      "  Batch 200 / 440.  Elapsed: 0:02:45.\n",
      "Training loss: 0.291\n",
      "  Batch 220 / 440.  Elapsed: 0:03:01.\n",
      "Training loss: 0.319\n",
      "  Batch 240 / 440.  Elapsed: 0:03:18.\n",
      "Training loss: 0.258\n",
      "  Batch 260 / 440.  Elapsed: 0:03:34.\n",
      "Training loss: 0.291\n",
      "  Batch 280 / 440.  Elapsed: 0:03:50.\n",
      "Training loss: 0.286\n",
      "  Batch 300 / 440.  Elapsed: 0:04:07.\n",
      "Training loss: 0.320\n",
      "  Batch 320 / 440.  Elapsed: 0:04:23.\n",
      "Training loss: 0.252\n",
      "  Batch 340 / 440.  Elapsed: 0:04:40.\n",
      "Training loss: 0.307\n",
      "  Batch 360 / 440.  Elapsed: 0:04:56.\n",
      "Training loss: 0.275\n",
      "  Batch 380 / 440.  Elapsed: 0:05:13.\n",
      "Training loss: 0.327\n",
      "  Batch 400 / 440.  Elapsed: 0:05:29.\n",
      "Training loss: 0.263\n",
      "  Batch 420 / 440.  Elapsed: 0:05:45.\n",
      "Training loss: 0.318\n",
      "  Training epoch took: 0:06:01\n",
      "\n",
      "Running Validation...\n",
      "Epoch 6: Train Loss: 0.2893, Validation Loss: 0.2820\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch 20 / 440.  Elapsed: 0:00:17.\n",
      "Training loss: 0.266\n",
      "  Batch 40 / 440.  Elapsed: 0:00:34.\n",
      "Training loss: 0.273\n",
      "  Batch 60 / 440.  Elapsed: 0:00:50.\n",
      "Training loss: 0.281\n",
      "  Batch 80 / 440.  Elapsed: 0:01:07.\n",
      "Training loss: 0.306\n",
      "  Batch 100 / 440.  Elapsed: 0:01:23.\n",
      "Training loss: 0.334\n",
      "  Batch 120 / 440.  Elapsed: 0:01:40.\n",
      "Training loss: 0.258\n",
      "  Batch 140 / 440.  Elapsed: 0:01:56.\n",
      "Training loss: 0.295\n",
      "  Batch 160 / 440.  Elapsed: 0:02:12.\n",
      "Training loss: 0.288\n",
      "  Batch 180 / 440.  Elapsed: 0:02:29.\n",
      "Training loss: 0.275\n",
      "  Batch 200 / 440.  Elapsed: 0:02:45.\n",
      "Training loss: 0.271\n",
      "  Batch 220 / 440.  Elapsed: 0:03:02.\n",
      "Training loss: 0.291\n",
      "  Batch 240 / 440.  Elapsed: 0:03:18.\n",
      "Training loss: 0.263\n",
      "  Batch 260 / 440.  Elapsed: 0:03:34.\n",
      "Training loss: 0.271\n",
      "  Batch 280 / 440.  Elapsed: 0:03:51.\n",
      "Training loss: 0.282\n",
      "  Batch 300 / 440.  Elapsed: 0:04:07.\n",
      "Training loss: 0.275\n",
      "  Batch 320 / 440.  Elapsed: 0:04:24.\n",
      "Training loss: 0.296\n",
      "  Batch 340 / 440.  Elapsed: 0:04:40.\n",
      "Training loss: 0.260\n",
      "  Batch 360 / 440.  Elapsed: 0:04:56.\n",
      "Training loss: 0.250\n",
      "  Batch 380 / 440.  Elapsed: 0:05:13.\n",
      "Training loss: 0.313\n",
      "  Batch 400 / 440.  Elapsed: 0:05:29.\n",
      "Training loss: 0.297\n",
      "  Batch 420 / 440.  Elapsed: 0:05:46.\n",
      "Training loss: 0.295\n",
      "  Training epoch took: 0:06:01\n",
      "\n",
      "Running Validation...\n",
      "Epoch 7: Train Loss: 0.2821, Validation Loss: 0.3249\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch 20 / 440.  Elapsed: 0:00:17.\n",
      "Training loss: 0.280\n",
      "  Batch 40 / 440.  Elapsed: 0:00:34.\n",
      "Training loss: 0.273\n",
      "  Batch 60 / 440.  Elapsed: 0:00:50.\n",
      "Training loss: 0.280\n",
      "  Batch 80 / 440.  Elapsed: 0:01:06.\n",
      "Training loss: 0.248\n",
      "  Batch 100 / 440.  Elapsed: 0:01:23.\n",
      "Training loss: 0.330\n",
      "  Batch 120 / 440.  Elapsed: 0:01:39.\n",
      "Training loss: 0.320\n",
      "  Batch 140 / 440.  Elapsed: 0:01:56.\n",
      "Training loss: 0.260\n",
      "  Batch 160 / 440.  Elapsed: 0:02:12.\n",
      "Training loss: 0.286\n",
      "  Batch 180 / 440.  Elapsed: 0:02:28.\n",
      "Training loss: 0.275\n",
      "  Batch 200 / 440.  Elapsed: 0:02:45.\n",
      "Training loss: 0.273\n",
      "  Batch 220 / 440.  Elapsed: 0:03:01.\n",
      "Training loss: 0.281\n",
      "  Batch 240 / 440.  Elapsed: 0:03:18.\n",
      "Training loss: 0.239\n",
      "  Batch 260 / 440.  Elapsed: 0:03:34.\n",
      "Training loss: 0.241\n",
      "  Batch 280 / 440.  Elapsed: 0:03:50.\n",
      "Training loss: 0.237\n",
      "  Batch 300 / 440.  Elapsed: 0:04:07.\n",
      "Training loss: 0.276\n",
      "  Batch 320 / 440.  Elapsed: 0:04:23.\n",
      "Training loss: 0.313\n",
      "  Batch 340 / 440.  Elapsed: 0:04:40.\n",
      "Training loss: 0.266\n",
      "  Batch 360 / 440.  Elapsed: 0:04:56.\n",
      "Training loss: 0.313\n",
      "  Batch 380 / 440.  Elapsed: 0:05:13.\n",
      "Training loss: 0.300\n",
      "  Batch 400 / 440.  Elapsed: 0:05:29.\n",
      "Training loss: 0.304\n",
      "  Batch 420 / 440.  Elapsed: 0:05:46.\n",
      "Training loss: 0.284\n",
      "  Training epoch took: 0:06:01\n",
      "\n",
      "Running Validation...\n",
      "Epoch 8: Train Loss: 0.2800, Validation Loss: 0.3164\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch 20 / 440.  Elapsed: 0:00:17.\n",
      "Training loss: 0.284\n",
      "  Batch 40 / 440.  Elapsed: 0:00:34.\n",
      "Training loss: 0.358\n",
      "  Batch 60 / 440.  Elapsed: 0:00:50.\n",
      "Training loss: 0.272\n",
      "  Batch 80 / 440.  Elapsed: 0:01:07.\n",
      "Training loss: 0.284\n",
      "  Batch 100 / 440.  Elapsed: 0:01:23.\n",
      "Training loss: 0.254\n",
      "  Batch 120 / 440.  Elapsed: 0:01:39.\n",
      "Training loss: 0.281\n",
      "  Batch 140 / 440.  Elapsed: 0:01:56.\n",
      "Training loss: 0.274\n",
      "  Batch 160 / 440.  Elapsed: 0:02:12.\n",
      "Training loss: 0.298\n",
      "  Batch 180 / 440.  Elapsed: 0:02:29.\n",
      "Training loss: 0.268\n",
      "  Batch 200 / 440.  Elapsed: 0:02:45.\n",
      "Training loss: 0.277\n",
      "  Batch 220 / 440.  Elapsed: 0:03:01.\n",
      "Training loss: 0.260\n",
      "  Batch 240 / 440.  Elapsed: 0:03:18.\n",
      "Training loss: 0.271\n",
      "  Batch 260 / 440.  Elapsed: 0:03:34.\n",
      "Training loss: 0.291\n",
      "  Batch 280 / 440.  Elapsed: 0:03:51.\n",
      "Training loss: 0.238\n",
      "  Batch 300 / 440.  Elapsed: 0:04:07.\n",
      "Training loss: 0.274\n",
      "  Batch 320 / 440.  Elapsed: 0:04:24.\n",
      "Training loss: 0.315\n",
      "  Batch 340 / 440.  Elapsed: 0:04:40.\n",
      "Training loss: 0.269\n",
      "  Batch 360 / 440.  Elapsed: 0:04:56.\n",
      "Training loss: 0.273\n",
      "  Batch 380 / 440.  Elapsed: 0:05:13.\n",
      "Training loss: 0.275\n",
      "  Batch 400 / 440.  Elapsed: 0:05:29.\n",
      "Training loss: 0.255\n",
      "  Batch 420 / 440.  Elapsed: 0:05:45.\n",
      "Training loss: 0.259\n",
      "  Training epoch took: 0:06:01\n",
      "\n",
      "Running Validation...\n",
      "Epoch 9: Train Loss: 0.2780, Validation Loss: 0.3131\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    store_train_loss = []\n",
    "    store_val_loss = []\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs_ids, attention_masks, labels = batch\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(inputs_ids, \n",
    "                    attention_mask=attention_masks)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        store_train_loss.append(loss.item())\n",
    "\n",
    "        if step % show_every == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {} / {}.  Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            print('Training loss: %.3f'%(np.mean(store_train_loss[-show_every:]) )) \n",
    "    train_losses.append(np.mean(store_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        val_inputs_ids, val_attention_masks, val_labels = batch\n",
    "        with torch.no_grad():  \n",
    "            val_outputs = model(val_inputs_ids,  \n",
    "                            attention_mask=val_attention_masks)\n",
    "        val_logits = val_outputs[0]\n",
    "        val_loss = F.mse_loss(val_outputs, val_labels)\n",
    "\n",
    "        store_val_loss.append(val_loss.item())\n",
    "    mean_val_loss = np.mean(store_val_loss)\n",
    "    val_losses.append(mean_val_loss)\n",
    "\n",
    "\n",
    "    print(\"Epoch {}: Train Loss: {:.4f}, Validation Loss: {:.4f}\".format(epoch_i, train_losses[-1], val_losses[-1]))\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T16:58:11.482859Z",
     "iopub.status.busy": "2023-05-12T16:58:11.482108Z",
     "iopub.status.idle": "2023-05-12T16:58:11.531991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        text_id                                          full_text  cohesion  \\\n",
      "0  0000C359D63E  when a person has no experience on a job their...         0   \n",
      "1  000BAD50D026  Do you think students would benefit from being...         0   \n",
      "2  00367BB2546B  Thomas Jefferson once states that \"it is wonde...         0   \n",
      "\n",
      "   syntax  vocabulary  phraseology  grammar  conventions  \n",
      "0       0           0            0        0            0  \n",
      "1       0           0            0        0            0  \n",
      "2       0           0            0        0            0  \n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(dirname + '/test.csv')\n",
    "test['cohesion'] = 0\n",
    "test['syntax'] = 0\n",
    "test['vocabulary'] = 0\n",
    "test['phraseology'] = 0\n",
    "test['grammar'] = 0\n",
    "test['conventions'] = 0\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T16:58:11.871296Z",
     "iopub.status.busy": "2023-05-12T16:58:11.870752Z",
     "iopub.status.idle": "2023-05-12T16:58:11.984611Z",
     "shell.execute_reply": "2023-05-12T16:58:11.983800Z",
     "shell.execute_reply.started": "2023-05-12T16:58:11.871259Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_dataset = CustomDataset(test, tokenizer)\n",
    "batch_size = 8\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "outputs = torch.zeros(len(test_dataset), 6)\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs_ids, attention_masks, labels = batch\n",
    "        outputs[step*batch_size:(step+1)*batch_size] = model(inputs_ids, attention_mask=attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T17:11:37.366848Z",
     "iopub.status.busy": "2023-05-12T17:11:37.365804Z",
     "iopub.status.idle": "2023-05-12T17:11:37.386686Z",
     "shell.execute_reply": "2023-05-12T17:11:37.385646Z",
     "shell.execute_reply.started": "2023-05-12T17:11:37.366802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n",
      "0  0000C359D63E  3.103090  2.989382    3.289109     3.112214  2.984290   \n",
      "1  000BAD50D026  3.052192  2.956181    3.237710     3.094946  3.002252   \n",
      "2  00367BB2546B  3.711895  3.552477    3.543804     3.622966  3.530829   \n",
      "\n",
      "   conventions  \n",
      "0     3.032859  \n",
      "1     3.056203  \n",
      "2     3.603334  \n"
     ]
    }
   ],
   "source": [
    "test['cohesion'] = outputs[:,0].detach().cpu().numpy()\n",
    "test['syntax'] = outputs[:,1].detach().cpu().numpy()\n",
    "test['vocabulary'] = outputs[:,2].detach().cpu().numpy()\n",
    "test['phraseology'] = outputs[:,3].detach().cpu().numpy()\n",
    "test['grammar'] = outputs[:,4].detach().cpu().numpy()\n",
    "test['conventions'] = outputs[:,5].detach().cpu().numpy()\n",
    "\n",
    "test.drop(columns=['full_text'], inplace=True)\n",
    "print(test.head())\n",
    "\n",
    "test.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T17:14:24.313553Z",
     "iopub.status.busy": "2023-05-12T17:14:24.313188Z",
     "iopub.status.idle": "2023-05-12T17:14:24.318155Z",
     "shell.execute_reply": "2023-05-12T17:14:24.317220Z",
     "shell.execute_reply.started": "2023-05-12T17:14:24.313524Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "class RobertaForMultipleRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaForMultipleRegression, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.regressor = nn.Linear(self.roberta.config.hidden_size, 6)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]  # استفاده از بردار [CLS]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        return self.regressor(pooled_output)\n",
    "\n",
    "epochs = 5\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=total_steps)\n",
    "\n",
    "def train_model():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_masks, labels = batch\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_masks)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_dataloader)\n",
    "\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, attention_masks, labels = batch\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks)\n",
    "            loss = F.mse_loss(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_dataloader)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model()\n",
    "    val_loss = evaluate_model()\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
